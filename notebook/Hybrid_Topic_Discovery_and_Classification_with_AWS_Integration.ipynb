{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eb2ca6b",
   "metadata": {
    "id": "4eb2ca6b"
   },
   "source": [
    "# Hybrid Topic Discovery & Classification with AWS Integration\n",
    "\n",
    "**Purpose**: Classify questions against existing topics and discover new topics using hybrid approach.\n",
    "\n",
    "**Data Flow**:\n",
    "1. Load topics from Google Sheets\n",
    "2. Load student questions from Langfuse CSV\n",
    "3. Similarity classification (threshold-based)\n",
    "4. Clustering for new topic discovery\n",
    "5. Output parquet files to AWS S3\n",
    "\n",
    "**Key Features**:\n",
    "- AWS S3 for embeddings cache and outputs\n",
    "- Environment-responsive configuration\n",
    "- Comprehensive error logging\n",
    "- Analytics outputs for Streamlit dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e89b193",
   "metadata": {
    "id": "3e89b193"
   },
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8a424b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "da8a424b",
    "outputId": "93c95577-ce1b-43d1-84e6-c148c376b8b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/154.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m154.7/154.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/140.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m140.6/140.6 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m109.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q openai pandas numpy scipy scikit-learn matplotlib seaborn tqdm umap-learn hdbscan bertopic backoff boto3 gspread oauth2client pyarrow fastparquet python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dca8fb7",
   "metadata": {
    "id": "3dca8fb7"
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa6d765",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1fa6d765",
    "outputId": "3bd00d21-7027-4f74-fb30-5fc3bb39b8ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Configuration loaded\n",
      "   Mode: all, Threshold: 0.7\n",
      "   S3 Bucket: byupathway-public\n",
      "   Embedding Model: text-embedding-3-small\n"
     ]
    }
   ],
   "source": [
    "# Processing settings\n",
    "EVAL_MODE = \"all\"  # \"sample\" or \"all\"\n",
    "SAMPLE_SIZE = 1000\n",
    "SIMILARITY_THRESHOLD = 0.70\n",
    "REPRESENTATIVE_QUESTION_METHOD = \"centroid\"  # \"centroid\" or \"frequent\"\n",
    "\n",
    "# Model settings\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "EMBEDDING_DIMENSIONS = 1536\n",
    "GPT_MODEL = \"gpt-5-nano\"\n",
    "\n",
    "# AWS S3 settings\n",
    "S3_BUCKET = \"byupathway-public\"\n",
    "S3_OUTPUT_PREFIX = \"topic-modeling-data\"\n",
    "S3_CACHE_PREFIX = \"embeddings-cache\"\n",
    "S3_REGION = \"us-east-1\"\n",
    "\n",
    "# Embedding storage settings\n",
    "EMBEDDING_STORAGE = \"local\"  # \"s3\" or \"local\"\n",
    "LOCAL_CACHE_DIR = \"./embedding_cache\"  # Directory for local embedding storage\n",
    "\n",
    "# Clustering settings\n",
    "UMAP_N_COMPONENTS = 5\n",
    "HDBSCAN_MIN_CLUSTER_SIZE = 3\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Google Sheets URL (This should come from Elder Edwards)\n",
    "GOOGLE_SHEETS_URL = \"https://docs.google.com/spreadsheets/d/1aX7ILPVAU_9MsliuXMeDstzWz5DqPLAY5LNBfV6l_NQ/\"\n",
    "\n",
    "print(\"âœ… Configuration loaded\")\n",
    "print(f\"   Mode: {EVAL_MODE}, Threshold: {SIMILARITY_THRESHOLD}\")\n",
    "print(f\"   S3 Bucket: {S3_BUCKET}\")\n",
    "print(f\"   Embedding Model: {EMBEDDING_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2988c527",
   "metadata": {
    "id": "2988c527"
   },
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89070b2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e89070b2",
    "outputId": "7448e5e3-251b-4ad7-f4d9-2bb8b9f7dae5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Running in Google Colab\n",
      "âœ… Environment setup complete\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import time\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Tuple, Optional, Any\n",
    "import asyncio\n",
    "import backoff\n",
    "import re\n",
    "import hashlib\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "\n",
    "# Detect environment\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    from google.colab import userdata\n",
    "    print(\"ðŸ”§ Running in Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    print(\"ðŸ”§ Running locally\")\n",
    "\n",
    "# Load credentials\n",
    "if IN_COLAB:\n",
    "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "    AWS_ACCESS_KEY = userdata.get('AWS_ACCESS_KEY_ID')\n",
    "    AWS_SECRET_KEY = userdata.get('AWS_SECRET_ACCESS_KEY')\n",
    "    GOOGLE_SERVICE_ACCOUNT = userdata.get('GOOGLE_SERVICE_ACCOUNT_JSON')\n",
    "else:\n",
    "    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "    AWS_ACCESS_KEY = os.getenv('AWS_ACCESS_KEY_ID')\n",
    "    AWS_SECRET_KEY = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "    GOOGLE_SERVICE_ACCOUNT = os.getenv('GOOGLE_SERVICE_ACCOUNT_JSON')\n",
    "\n",
    "# Initialize OpenAI\n",
    "from openai import OpenAI, AsyncOpenAI\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "async_client = AsyncOpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Initialize AWS S3\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=AWS_ACCESS_KEY,\n",
    "    aws_secret_access_key=AWS_SECRET_KEY,\n",
    "    region_name=S3_REGION\n",
    ")\n",
    "\n",
    "print(\"âœ… Environment setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6199ff1c",
   "metadata": {
    "id": "6199ff1c"
   },
   "source": [
    "## Error Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f9701f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "09f9701f",
    "outputId": "55f70dc0-665a-43a9-f804-2e759c8cedde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Error logger initialized\n"
     ]
    }
   ],
   "source": [
    "class ErrorLogger:\n",
    "    def __init__(self):\n",
    "        self.errors = []\n",
    "        self.warnings = []\n",
    "        self.rows_dropped = []\n",
    "        self.timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    def log_error(self, stage: str, message: str, data: Any = None):\n",
    "        entry = {\"timestamp\": datetime.now().isoformat(), \"stage\": stage, \"message\": message, \"data\": str(data)}\n",
    "        self.errors.append(entry)\n",
    "        print(f\"âŒ ERROR [{stage}]: {message}\")\n",
    "\n",
    "    def log_warning(self, stage: str, message: str, data: Any = None):\n",
    "        entry = {\"timestamp\": datetime.now().isoformat(), \"stage\": stage, \"message\": message, \"data\": str(data)}\n",
    "        self.warnings.append(entry)\n",
    "        print(f\"âš ï¸  WARNING [{stage}]: {message}\")\n",
    "\n",
    "    def log_dropped_row(self, stage: str, reason: str, row_data: Any):\n",
    "        entry = {\"timestamp\": datetime.now().isoformat(), \"stage\": stage, \"reason\": reason, \"row_data\": str(row_data)}\n",
    "        self.rows_dropped.append(entry)\n",
    "\n",
    "    def get_summary(self):\n",
    "        return {\n",
    "            \"total_errors\": len(self.errors),\n",
    "            \"total_warnings\": len(self.warnings),\n",
    "            \"total_dropped_rows\": len(self.rows_dropped),\n",
    "            \"errors\": self.errors,\n",
    "            \"warnings\": self.warnings,\n",
    "            \"dropped_rows\": self.rows_dropped\n",
    "        }\n",
    "\n",
    "    def save_to_file(self, filename: str):\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(self.get_summary(), f, indent=2)\n",
    "        print(f\"ðŸ“ Error log saved: {filename}\")\n",
    "        return filename\n",
    "\n",
    "error_logger = ErrorLogger()\n",
    "print(\"âœ… Error logger initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40fc8e3",
   "metadata": {
    "id": "e40fc8e3"
   },
   "source": [
    "## AWS S3 Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3d272b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2e3d272b",
    "outputId": "c6ab4a98-b2bd-4ca7-f334-7d04986e1db2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… AWS S3 utilities ready (with retry logic)\n"
     ]
    }
   ],
   "source": [
    "@backoff.on_exception(\n",
    "    backoff.expo,\n",
    "    Exception,\n",
    "    max_tries=5,\n",
    "    max_time=30,\n",
    "    giveup=lambda e: isinstance(e, (KeyboardInterrupt, SystemExit))\n",
    ")\n",
    "def upload_to_s3(local_path: str, s3_key: str, public: bool = True) -> bool:\n",
    "    \"\"\"Upload file to S3 with retry logic and exponential backoff\n",
    "\n",
    "    Args:\n",
    "        local_path: Local file path to upload\n",
    "        s3_key: S3 key (path in bucket)\n",
    "        public: Whether to set public-read ACL (default True)\n",
    "\n",
    "    Returns:\n",
    "        bool: True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        extra_args = {'ACL': 'public-read'} if public else {}\n",
    "        s3_client.upload_file(local_path, S3_BUCKET, s3_key, ExtraArgs=extra_args)\n",
    "\n",
    "        if public:\n",
    "            url = f\"https://{S3_BUCKET}.s3.amazonaws.com/{s3_key}\"\n",
    "            print(f\"âœ… Uploaded to S3: {url}\")\n",
    "        else:\n",
    "            print(f\"âœ… Uploaded to S3: s3://{S3_BUCKET}/{s3_key}\")\n",
    "\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        error_logger.log_error(\"S3_Upload\", f\"Failed to upload {local_path} after retries\", e)\n",
    "        return False\n",
    "\n",
    "@backoff.on_exception(\n",
    "    backoff.expo,\n",
    "    Exception,\n",
    "    max_tries=3,\n",
    "    max_time=15,\n",
    "    giveup=lambda e: isinstance(e, (KeyboardInterrupt, SystemExit))\n",
    ")\n",
    "def download_from_s3(s3_key: str, local_path: str) -> bool:\n",
    "    \"\"\"Download file from S3 with retry logic\"\"\"\n",
    "    try:\n",
    "        s3_client.download_file(S3_BUCKET, s3_key, local_path)\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == '404':\n",
    "            # File doesn't exist - don't retry\n",
    "            return False\n",
    "        error_logger.log_error(\"S3_Download\", f\"Failed to download {s3_key} after retries\", e)\n",
    "        return False\n",
    "\n",
    "def delete_s3_folder(prefix: str):\n",
    "    \"\"\"Delete all objects with given prefix\"\"\"\n",
    "    try:\n",
    "        response = s3_client.list_objects_v2(Bucket=S3_BUCKET, Prefix=prefix)\n",
    "        if 'Contents' in response:\n",
    "            objects = [{'Key': obj['Key']} for obj in response['Contents']]\n",
    "            s3_client.delete_objects(Bucket=S3_BUCKET, Delete={'Objects': objects})\n",
    "            print(f\"ðŸ—‘ï¸  Deleted {len(objects)} objects from s3://{S3_BUCKET}/{prefix}\")\n",
    "    except Exception as e:\n",
    "        error_logger.log_error(\"S3_Delete\", f\"Failed to delete folder {prefix}\", e)\n",
    "\n",
    "print(\"âœ… AWS S3 utilities ready (with retry logic)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mzynRnU-5AVW",
   "metadata": {
    "id": "mzynRnU-5AVW"
   },
   "source": [
    "# Master File Sync System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "E65Qd05t5FWu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E65Qd05t5FWu",
    "outputId": "e50af6af-85ab-4ebd-89e5-f06e32a6dbb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Master file sync system ready (S3-only)\n",
      "   Storage: s3://byupathway-public/langfuse/langfuse_traces_master.parquet\n"
     ]
    }
   ],
   "source": [
    "# S3 Master file settings\n",
    "S3_MASTER_KEY = \"langfuse/langfuse_traces_master.parquet\"\n",
    "\n",
    "def sync_master_file(new_data_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Sync master file with S3 storage\n",
    "\n",
    "    Flow:\n",
    "    1. Download master file from S3 (if exists)\n",
    "    2. Merge new data with master\n",
    "    3. Remove duplicates (question/input + timestamp)\n",
    "    4. Upload updated master back to S3\n",
    "\n",
    "    Args:\n",
    "        new_data_df: New data to merge with master\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Complete master dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\nðŸ”„ Syncing master file with S3...\")\n",
    "\n",
    "    local_master_path = \"/tmp/langfuse_traces_master.parquet\"\n",
    "    master_df = None\n",
    "\n",
    "    # Step 1: Try to load from S3\n",
    "    print(\"ðŸ“‚ Checking S3 for master file...\")\n",
    "    if download_from_s3(S3_MASTER_KEY, local_master_path):\n",
    "        try:\n",
    "            master_df = pd.read_parquet(local_master_path)\n",
    "            print(f\"âœ… Loaded master from S3: {len(master_df)} rows\")\n",
    "            print(f\"   Timestamp range: {master_df['timestamp'].min()} to {master_df['timestamp'].max()}\")\n",
    "        except Exception as e:\n",
    "            error_logger.log_error(\"MasterSync\", \"Failed to read master from S3\", e)\n",
    "            master_df = None\n",
    "\n",
    "    # Step 2: Merge new data with master\n",
    "    if master_df is None:\n",
    "        print(\"ðŸ“ No existing master file found - creating new one\")\n",
    "        merged_df = new_data_df.copy()\n",
    "    else:\n",
    "        print(f\"ðŸ”€ Merging new data ({len(new_data_df)} rows) with master ({len(master_df)} rows)...\")\n",
    "\n",
    "        # Ensure both have same columns\n",
    "        for col in new_data_df.columns:\n",
    "            if col not in master_df.columns:\n",
    "                master_df[col] = None\n",
    "        for col in master_df.columns:\n",
    "            if col not in new_data_df.columns:\n",
    "                new_data_df[col] = None\n",
    "\n",
    "        # Combine\n",
    "        merged_df = pd.concat([master_df, new_data_df], ignore_index=True)\n",
    "        print(f\"   Combined: {len(merged_df)} rows\")\n",
    "\n",
    "    # Step 3: Remove duplicates (question/input + timestamp REQUIRED, id optional)\n",
    "    print(\"ðŸ§¹ Removing duplicates (same question + timestamp)...\")\n",
    "    before_dedup = len(merged_df)\n",
    "\n",
    "    # Determine question column name (could be 'question' or 'input')\n",
    "    question_col = 'question' if 'question' in merged_df.columns else 'input' if 'input' in merged_df.columns else None\n",
    "\n",
    "    # Check for required columns\n",
    "    if question_col is None or 'timestamp' not in merged_df.columns:\n",
    "        print(\"   âš ï¸  Warning: Missing required columns (question/input and timestamp) for deduplication\")\n",
    "    else:\n",
    "        # Prefer rows with richer feedback metadata when duplicate keys exist\n",
    "        priority_cols = []\n",
    "        if 'feedback_comment' in merged_df.columns:\n",
    "            merged_df['_has_feedback_comment'] = merged_df['feedback_comment'].notna() & (merged_df['feedback_comment'].astype(str).str.strip() != '')\n",
    "            priority_cols.append('_has_feedback_comment')\n",
    "        if 'user_feedback' in merged_df.columns:\n",
    "            merged_df['_has_user_feedback'] = merged_df['user_feedback'].notna() & (merged_df['user_feedback'].astype(str).str.strip() != '')\n",
    "            priority_cols.append('_has_user_feedback')\n",
    "\n",
    "        if priority_cols:\n",
    "            merged_df = merged_df.sort_values(by=priority_cols, ascending=False)\n",
    "\n",
    "        # Deduplicate by question/input + timestamp only\n",
    "        # keep='first' now preserves the richest row after priority sort\n",
    "        merged_df = merged_df.drop_duplicates(subset=[question_col, 'timestamp'], keep='first')\n",
    "\n",
    "        # Cleanup temp priority columns\n",
    "        for col in ['_has_feedback_comment', '_has_user_feedback']:\n",
    "            if col in merged_df.columns:\n",
    "                merged_df = merged_df.drop(columns=[col])\n",
    "\n",
    "        after_dedup = len(merged_df)\n",
    "        print(f\"   Removed {before_dedup - after_dedup} duplicates\")\n",
    "        print(f\"   Deduplication criteria: {question_col} + timestamp (preferring richer feedback metadata)\")\n",
    "        print(f\"   Final: {len(merged_df)} rows\")\n",
    "\n",
    "    # Sort by timestamp (newest first)\n",
    "    if 'timestamp' in merged_df.columns:\n",
    "        merged_df = merged_df.sort_values('timestamp', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # Step 4: Upload to S3\n",
    "    print(\"â˜ï¸  Uploading to S3...\")\n",
    "    merged_df.to_parquet(local_master_path)\n",
    "    s3_success = upload_to_s3(local_master_path, S3_MASTER_KEY, public=False)\n",
    "\n",
    "    # Clean up temp file\n",
    "    try:\n",
    "        os.unlink(local_master_path)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    if s3_success:\n",
    "        print(f\"âœ… Master file synced: {len(merged_df)} total rows\")\n",
    "    else:\n",
    "        print(f\"âŒ S3 upload failed - master file not backed up!\")\n",
    "        error_logger.log_error(\"MasterSync\", \"Failed to upload master file to S3\", None)\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "print(\"âœ… Master file sync system ready (S3-only)\")\n",
    "print(f\"   Storage: s3://{S3_BUCKET}/{S3_MASTER_KEY}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f833c4",
   "metadata": {
    "id": "f4f833c4"
   },
   "source": [
    "## Google Sheets Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ded2f35",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ded2f35",
    "outputId": "3a89f05a-5e42-4d3f-b29b-241b836e26ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Google Sheets integration ready\n"
     ]
    }
   ],
   "source": [
    "def read_topics_from_google_sheets(sheet_url: str) -> pd.DataFrame:\n",
    "    \"\"\"Read topics from Google Sheets with flexible column handling\"\"\"\n",
    "    try:\n",
    "        creds_dict = json.loads(GOOGLE_SERVICE_ACCOUNT)\n",
    "        scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']\n",
    "        creds = ServiceAccountCredentials.from_json_keyfile_dict(creds_dict, scope)\n",
    "        gc = gspread.authorize(creds)\n",
    "\n",
    "        sheet = gc.open_by_url(sheet_url)\n",
    "        worksheet = sheet.get_worksheet(0)\n",
    "        data = worksheet.get_all_records()\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        # Handle both uppercase and lowercase column names with flexible matching\n",
    "        column_mapping = {}\n",
    "        for col in df.columns:\n",
    "            col_lower = col.lower().strip()\n",
    "            # Match topic/topics\n",
    "            if col_lower in ['topics', 'topic']:\n",
    "                column_mapping[col] = 'topic'\n",
    "            # Match subtopic/subtopics\n",
    "            elif col_lower in ['subtopics', 'subtopic']:\n",
    "                column_mapping[col] = 'subtopic'\n",
    "            # Match question/questions/representative question/representative questions\n",
    "            elif col_lower in ['questions', 'question', 'representative question', 'representative questions']:\n",
    "                column_mapping[col] = 'question'\n",
    "\n",
    "        df = df.rename(columns=column_mapping)\n",
    "\n",
    "        required = ['topic', 'subtopic', 'question']\n",
    "        if not all(col in df.columns for col in required):\n",
    "            raise ValueError(f\"Missing required columns. Found: {list(df.columns)}\")\n",
    "\n",
    "        df = df[required].dropna()\n",
    "        print(f\"âœ… Loaded {len(df)} topics from Google Sheets\")\n",
    "        print(f\"   Unique topics: {df['topic'].nunique()}, Unique subtopics: {df['subtopic'].nunique()}\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        error_logger.log_error(\"GoogleSheets\", \"Failed to read topics\", e)\n",
    "        raise\n",
    "\n",
    "print(\"âœ… Google Sheets integration ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c2f81d",
   "metadata": {
    "id": "01c2f81d"
   },
   "source": [
    "## Langfuse Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c220e2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "41c220e2",
    "outputId": "6d5136ff-1bd0-440e-b174-470fca18e562"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Malformed row filter and ACM detector ready\n",
      "âœ… Langfuse cleaning utilities ready\n"
     ]
    }
   ],
   "source": [
    "def filter_malformed_rows(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter out malformed rows containing both 'kwargs' AND 'args' (case-insensitive)\n",
    "    This is the FIRST step in data cleaning to remove Langfuse error rows\n",
    "    \"\"\"\n",
    "    print(f\"ðŸ” Filtering malformed rows (containing 'kwargs' AND 'args')...\")\n",
    "    before_filter = len(df)\n",
    "\n",
    "    # Convert all columns to string and check for both kwargs and args (case-insensitive)\n",
    "    df_str = df.astype(str)\n",
    "\n",
    "    # Check each row for both terms\n",
    "    malformed_mask = df_str.apply(\n",
    "        lambda row: any('kwargs' in str(val).lower() for val in row) and\n",
    "                   any('args' in str(val).lower() for val in row),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Keep only rows that are NOT malformed\n",
    "    filtered_df = df[~malformed_mask].copy()\n",
    "\n",
    "    after_filter = len(filtered_df)\n",
    "    filtered_count = before_filter - after_filter\n",
    "\n",
    "    print(f\"âœ… Filtered {filtered_count} malformed rows ({filtered_count/before_filter*100:.1f}%)\")\n",
    "    print(f\"   Remaining: {after_filter} rows\")\n",
    "\n",
    "    # Log dropped rows\n",
    "    for idx in df[malformed_mask].index:\n",
    "        error_logger.log_dropped_row(\n",
    "            \"MalformedFilter\",\n",
    "            \"Row contains both 'kwargs' and 'args' - likely Langfuse error\",\n",
    "            {\"index\": idx}\n",
    "        )\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "def detect_acm_question(question: str) -> bool:\n",
    "    \"\"\"Detect if question has ACM prefix\"\"\"\n",
    "    if not isinstance(question, str):\n",
    "        return False\n",
    "\n",
    "    pattern = r'^\\s*\\(ACMs?\\s+[Qq]uestion\\)\\s*:?\\s*'\n",
    "    return bool(re.search(pattern, question, flags=re.IGNORECASE))\n",
    "\n",
    "print(\"âœ… Malformed row filter and ACM detector ready\")\n",
    "\n",
    "\n",
    "def clean_langfuse_data(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Clean and validate Langfuse data with comprehensive error handling.\n",
    "\n",
    "    Filters out non-chat traces (e.g. general_feedback) and extracts all\n",
    "    available fields including latency, cost, session_id, user_id, tags, scores.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (cleaned_questions_df, general_feedback_df)\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"ðŸ§¹ Cleaning {len(df)} Langfuse rows...\")\n",
    "\n",
    "    # Separate general feedback from chat traces BEFORE cleaning\n",
    "    general_feedback_rows = []\n",
    "    name_col_exists = 'name' in df.columns\n",
    "\n",
    "    if name_col_exists:\n",
    "        non_chat_mask = df['name'].notna() & (df['name'] != 'chat')\n",
    "        non_chat_df = df[non_chat_mask]\n",
    "        chat_df = df[~non_chat_mask]\n",
    "        print(f\"   Separated {len(non_chat_df)} non-chat traces (general feedback, etc.)\")\n",
    "        print(f\"   Processing {len(chat_df)} chat traces...\")\n",
    "\n",
    "        # Collect general feedback entries\n",
    "        for _, row in non_chat_df.iterrows():\n",
    "            fb_entry = {\n",
    "                'id': row.get('id', None),\n",
    "                'name': row.get('name', None),\n",
    "                'input': row.get('input', None),\n",
    "                'output': row.get('output', None),\n",
    "                'timestamp': row.get('timestamp', None),\n",
    "                'user_id': row.get('user_id', None),\n",
    "                'session_id': row.get('session_id', None),\n",
    "                'tags': row.get('tags', None),\n",
    "            }\n",
    "            general_feedback_rows.append(fb_entry)\n",
    "    else:\n",
    "        chat_df = df\n",
    "        print(\"   No 'name' column found - processing all rows as chat traces\")\n",
    "\n",
    "    general_feedback_df = pd.DataFrame(general_feedback_rows) if general_feedback_rows else pd.DataFrame()\n",
    "    if not general_feedback_df.empty:\n",
    "        print(f\"   ðŸ“ General feedback entries: {len(general_feedback_df)}\")\n",
    "\n",
    "    cleaned_rows = []\n",
    "\n",
    "    for idx, row in enumerate(chat_df.itertuples(index=False), 1):\n",
    "        try:\n",
    "            cleaned_row = {}\n",
    "\n",
    "            # 0. Extract id (needed for deduplication)\n",
    "            id_value = getattr(row, 'id', None)\n",
    "            cleaned_row['id'] = id_value if pd.notna(id_value) else None\n",
    "\n",
    "            # 1. Extract timestamp\n",
    "            timestamp_value = getattr(row, 'timestamp', None)\n",
    "            if pd.notna(timestamp_value):\n",
    "                try:\n",
    "                    # Handle different timestamp formats\n",
    "                    if isinstance(timestamp_value, str):\n",
    "                        # Try parsing as ISO format first\n",
    "                        cleaned_row['timestamp'] = pd.to_datetime(timestamp_value).isoformat()\n",
    "                    else:\n",
    "                        cleaned_row['timestamp'] = pd.to_datetime(timestamp_value).isoformat()\n",
    "                except (ValueError, TypeError):\n",
    "                    # If parsing fails, try to extract from kwargs\n",
    "                    if hasattr(row, 'kwargs') and pd.notna(getattr(row, 'kwargs', None)):\n",
    "                        kwargs_str = str(getattr(row, 'kwargs', ''))\n",
    "                        # Look for timestamp in kwargs\n",
    "                        ts_match = re.search(r'\"timestamp\"\\s*:\\s*\"([^\"]*)\"', kwargs_str)\n",
    "                        if ts_match:\n",
    "                            try:\n",
    "                                cleaned_row['timestamp'] = pd.to_datetime(ts_match.group(1)).isoformat()\n",
    "                            except:\n",
    "                                cleaned_row['timestamp'] = None\n",
    "                        else:\n",
    "                            cleaned_row['timestamp'] = None\n",
    "                    else:\n",
    "                        cleaned_row['timestamp'] = None\n",
    "            else:\n",
    "                cleaned_row['timestamp'] = None\n",
    "\n",
    "            # 2. Extract input (malformed rows already filtered out)\n",
    "            input_value = getattr(row, 'input', None)\n",
    "\n",
    "            # Validate input is not empty\n",
    "            if pd.isna(input_value) or str(input_value).strip() == '':\n",
    "                error_logger.log_dropped_row(\"LangfuseClean\", \"Missing input\", {\"index\": idx})\n",
    "                continue\n",
    "\n",
    "            cleaned_row['input'] = str(input_value).strip()\n",
    "\n",
    "            # Detect if this is an ACM question (before cleaning the prefix)\n",
    "            cleaned_row['is_acm_question'] = detect_acm_question(cleaned_row['input'])\n",
    "\n",
    "            # 3. Extract output (no kwargs handling needed)\n",
    "            output_value = getattr(row, 'output', None)\n",
    "            cleaned_row['output'] = output_value if pd.notna(output_value) else None\n",
    "\n",
    "            # 4. Extract feedback label and reason (new Langfuse format)\n",
    "            feedback_label_raw = getattr(row, 'feedback_value', None)\n",
    "            legacy_feedback_raw = getattr(row, 'user_feedback', None)\n",
    "\n",
    "            # Prefer feedback_value (Good/Bad) when available\n",
    "            if pd.notna(feedback_label_raw) and str(feedback_label_raw).strip() != '':\n",
    "                cleaned_row['user_feedback'] = str(feedback_label_raw).strip()\n",
    "            elif pd.notna(legacy_feedback_raw) and str(legacy_feedback_raw).strip() != '':\n",
    "                legacy_text = str(legacy_feedback_raw).strip()\n",
    "                cleaned_row['user_feedback'] = legacy_text.split(':', 1)[0].strip() if ':' in legacy_text else legacy_text\n",
    "            else:\n",
    "                cleaned_row['user_feedback'] = None\n",
    "\n",
    "            # Feedback reason/comment (used for thumbs-down explanation)\n",
    "            feedback_comment_raw = getattr(row, 'feedback_comment', None)\n",
    "            if pd.notna(feedback_comment_raw) and str(feedback_comment_raw).strip() != '':\n",
    "                cleaned_row['feedback_comment'] = str(feedback_comment_raw).strip()\n",
    "            elif pd.notna(legacy_feedback_raw) and ':' in str(legacy_feedback_raw):\n",
    "                legacy_reason = str(legacy_feedback_raw).split(':', 1)[1].strip()\n",
    "                cleaned_row['feedback_comment'] = legacy_reason if legacy_reason else None\n",
    "            else:\n",
    "                cleaned_row['feedback_comment'] = None\n",
    "\n",
    "            # 5. Extract latency (seconds)\n",
    "            latency_value = getattr(row, 'latency', None)\n",
    "            if pd.notna(latency_value):\n",
    "                try:\n",
    "                    cleaned_row['latency'] = float(latency_value)\n",
    "                except (ValueError, TypeError):\n",
    "                    cleaned_row['latency'] = None\n",
    "            else:\n",
    "                cleaned_row['latency'] = None\n",
    "\n",
    "            # 6. Extract total_cost\n",
    "            cost_value = getattr(row, 'total_cost', None)\n",
    "            if pd.notna(cost_value):\n",
    "                try:\n",
    "                    cleaned_row['total_cost'] = float(cost_value)\n",
    "                except (ValueError, TypeError):\n",
    "                    cleaned_row['total_cost'] = None\n",
    "            else:\n",
    "                cleaned_row['total_cost'] = None\n",
    "\n",
    "            # 7. Extract session_id\n",
    "            session_value = getattr(row, 'session_id', None)\n",
    "            cleaned_row['session_id'] = session_value if pd.notna(session_value) else None\n",
    "\n",
    "            # 8. Extract user_id\n",
    "            user_id_value = getattr(row, 'user_id', None)\n",
    "            cleaned_row['user_id'] = user_id_value if pd.notna(user_id_value) else None\n",
    "\n",
    "            # 9. Extract tags (stored as JSON string list)\n",
    "            tags_value = getattr(row, 'tags', None)\n",
    "            if pd.notna(tags_value) and str(tags_value).strip() not in ('', '[]'):\n",
    "                cleaned_row['tags'] = str(tags_value)\n",
    "            else:\n",
    "                cleaned_row['tags'] = None\n",
    "\n",
    "            # 10. Extract scores (stored as JSON string list)\n",
    "            scores_value = getattr(row, 'scores', None)\n",
    "            if pd.notna(scores_value) and str(scores_value).strip() not in ('', '[]'):\n",
    "                cleaned_row['scores'] = str(scores_value)\n",
    "            else:\n",
    "                cleaned_row['scores'] = None\n",
    "\n",
    "            # 11. Extract release (git commit hash)\n",
    "            release_value = getattr(row, 'release', None)\n",
    "            cleaned_row['release'] = release_value if pd.notna(release_value) else None\n",
    "\n",
    "            # 12. Parse metadata JSON for country, state, city, language, is_suspicious, role\n",
    "            metadata_value = getattr(row, 'metadata', None)\n",
    "            if pd.notna(metadata_value) and metadata_value != '':\n",
    "                try:\n",
    "                    metadata = json.loads(metadata_value) if isinstance(metadata_value, str) else metadata_value\n",
    "\n",
    "                    # Extract geographic data (handle both flat and nested geo_data formats)\n",
    "                    geo_data = metadata.get('geo_data', {})\n",
    "                    if isinstance(geo_data, dict) and geo_data:\n",
    "                        cleaned_row['country'] = geo_data.get('country')\n",
    "                        cleaned_row['state'] = geo_data.get('state')\n",
    "                        cleaned_row['city'] = geo_data.get('city')\n",
    "                    else:\n",
    "                        # Fallback to flat metadata keys\n",
    "                        cleaned_row['country'] = metadata.get('country')\n",
    "                        cleaned_row['state'] = metadata.get('state')\n",
    "                        cleaned_row['city'] = metadata.get('city')\n",
    "\n",
    "                    cleaned_row['user_language'] = metadata.get('user_language')\n",
    "                    cleaned_row['role'] = metadata.get('role')\n",
    "\n",
    "                    # Extract is_suspicious from security_validation\n",
    "                    security_val = metadata.get('security_validation', {})\n",
    "                    if isinstance(security_val, dict):\n",
    "                        cleaned_row['is_suspicious'] = security_val.get('is_suspicious', False)\n",
    "                    else:\n",
    "                        cleaned_row['is_suspicious'] = False\n",
    "\n",
    "                except (json.JSONDecodeError, TypeError, AttributeError) as e:\n",
    "                    # Metadata parsing failed, set nulls\n",
    "                    error_logger.log_error(\"LangfuseClean\", f\"Failed to parse metadata at row {idx}\", e)\n",
    "                    cleaned_row['country'] = None\n",
    "                    cleaned_row['state'] = None\n",
    "                    cleaned_row['city'] = None\n",
    "                    cleaned_row['user_language'] = None\n",
    "                    cleaned_row['role'] = None\n",
    "                    cleaned_row['is_suspicious'] = False\n",
    "            else:\n",
    "                # No metadata, set nulls\n",
    "                cleaned_row['country'] = None\n",
    "                cleaned_row['state'] = None\n",
    "                cleaned_row['city'] = None\n",
    "                cleaned_row['user_language'] = None\n",
    "                cleaned_row['role'] = None\n",
    "                cleaned_row['is_suspicious'] = False\n",
    "\n",
    "            cleaned_rows.append(cleaned_row)\n",
    "\n",
    "        except Exception as e:\n",
    "            error_logger.log_dropped_row(\"LangfuseClean\", f\"Unexpected error: {e}\", {\"index\": idx})\n",
    "            continue\n",
    "\n",
    "    cleaned_df = pd.DataFrame(cleaned_rows)\n",
    "\n",
    "    # Remove duplicates (same timestamp and input)\n",
    "    if 'timestamp' in cleaned_df.columns and 'input' in cleaned_df.columns:\n",
    "        before_dedup = len(cleaned_df)\n",
    "        cleaned_df = cleaned_df.drop_duplicates(subset=['timestamp', 'input'], keep='first')\n",
    "        after_dedup = len(cleaned_df)\n",
    "        if before_dedup > after_dedup:\n",
    "            print(f\"ðŸ—‘ï¸  Removed {before_dedup - after_dedup} duplicate rows (same timestamp + input)\")\n",
    "\n",
    "    # Clean question prefixes (ACM Question)\n",
    "    cleaned_df['input'] = cleaned_df['input'].apply(lambda x: clean_question(x) if pd.notna(x) else x)\n",
    "\n",
    "    print(f\"âœ… Cleaned data: {len(cleaned_df)} rows ({len(chat_df) - len(cleaned_df)} dropped)\")\n",
    "    print(f\"   Columns: {list(cleaned_df.columns)}\")\n",
    "    print(f\"   Country data: {cleaned_df['country'].notna().sum()} rows with country info\")\n",
    "    print(f\"   With latency: {cleaned_df['latency'].notna().sum()}, With cost: {cleaned_df['total_cost'].notna().sum()}\")\n",
    "    print(f\"   With session_id: {cleaned_df['session_id'].notna().sum()}, With user_id: {cleaned_df['user_id'].notna().sum()}\")\n",
    "\n",
    "    return cleaned_df, general_feedback_df\n",
    "\n",
    "print(\"âœ… Langfuse cleaning utilities ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5469ad7d",
   "metadata": {
    "id": "5469ad7d"
   },
   "source": [
    "## Question Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c50dc2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "44c50dc2",
    "outputId": "238d413a-8b25-42af-be90-8406f5d79a36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Question preprocessing ready\n"
     ]
    }
   ],
   "source": [
    "def clean_question(question: str) -> str:\n",
    "    \"\"\"Remove ACM prefixes and clean text\"\"\"\n",
    "    if not isinstance(question, str):\n",
    "        return str(question) if question is not None else \"\"\n",
    "\n",
    "    pattern = r'^\\s*\\(ACMs?\\s+[Qq]uestion\\)\\s*:?\\s*'\n",
    "    cleaned = re.sub(pattern, '', question, flags=re.IGNORECASE).strip()\n",
    "    return cleaned if cleaned else question\n",
    "\n",
    "def preprocess_dataframe(df: pd.DataFrame, question_col: str) -> pd.DataFrame:\n",
    "    \"\"\"Apply cleaning to question column\"\"\"\n",
    "    df = df.copy()\n",
    "    df[question_col] = df[question_col].apply(clean_question)\n",
    "    return df\n",
    "\n",
    "print(\"âœ… Question preprocessing ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2fddd4",
   "metadata": {
    "id": "0c2fddd4"
   },
   "source": [
    "## S3 Embeddings Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccae5ebc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ccae5ebc",
    "outputId": "363b1c24-19d5-45ad-ef2c-aca8692de487"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Embeddings cache ready (with retry logic)\n",
      "   Storage: local\n"
     ]
    }
   ],
   "source": [
    "def get_cache_key(text: str, model: str) -> str:\n",
    "    \"\"\"Generate S3 cache key for text\"\"\"\n",
    "    text_hash = hashlib.md5(text.encode()).hexdigest()[:12]\n",
    "    return f\"{S3_CACHE_PREFIX}/{model}/{text_hash}.pkl\"\n",
    "\n",
    "def load_embedding_from_s3(text: str, model: str) -> Optional[List[float]]:\n",
    "    \"\"\"Load cached embedding from S3\"\"\"\n",
    "    cache_key = get_cache_key(text, model)\n",
    "    local_path = f\"/tmp/{cache_key.split('/')[-1]}\"\n",
    "\n",
    "    if download_from_s3(cache_key, local_path):\n",
    "        try:\n",
    "            with open(local_path, 'rb') as f:\n",
    "                embedding = pickle.load(f)\n",
    "            # Clean up temp file after reading\n",
    "            try:\n",
    "                os.unlink(local_path)\n",
    "            except:\n",
    "                pass\n",
    "            return embedding\n",
    "        except Exception as e:\n",
    "            # Clean up corrupted cache file\n",
    "            try:\n",
    "                os.unlink(local_path)\n",
    "            except:\n",
    "                pass\n",
    "    return None\n",
    "\n",
    "def save_embedding_to_s3(text: str, model: str, embedding: List[float]):\n",
    "    \"\"\"Save embedding to S3 cache with retry logic\"\"\"\n",
    "    cache_key = get_cache_key(text, model)\n",
    "    local_path = f\"/tmp/{cache_key.split('/')[-1]}\"\n",
    "\n",
    "    try:\n",
    "        # Write to local temp file\n",
    "        with open(local_path, 'wb') as f:\n",
    "            pickle.dump(embedding, f)\n",
    "\n",
    "        # Upload to S3 (with retry logic from upload_to_s3)\n",
    "        # Use public=False for cache files (no need for public access)\n",
    "        success = upload_to_s3(local_path, cache_key, public=False)\n",
    "\n",
    "        # Clean up temp file\n",
    "        try:\n",
    "            os.unlink(local_path)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        return success\n",
    "    except Exception as e:\n",
    "        # Clean up on failure\n",
    "        try:\n",
    "            if os.path.exists(local_path):\n",
    "                os.unlink(local_path)\n",
    "        except:\n",
    "            pass\n",
    "        # Don't log - cache failures are expected and handled gracefully\n",
    "        return False\n",
    "\n",
    "def get_local_cache_path(text: str, model: str) -> str:\n",
    "    \"\"\"Generate local cache path for text\"\"\"\n",
    "    text_hash = hashlib.md5(text.encode()).hexdigest()[:12]\n",
    "    os.makedirs(LOCAL_CACHE_DIR, exist_ok=True)\n",
    "    return f\"{LOCAL_CACHE_DIR}/{model}_{text_hash}.pkl\"\n",
    "\n",
    "def load_embedding_from_local(text: str, model: str) -> Optional[List[float]]:\n",
    "    \"\"\"Load cached embedding from local storage\"\"\"\n",
    "    cache_path = get_local_cache_path(text, model)\n",
    "\n",
    "    if os.path.exists(cache_path):\n",
    "        try:\n",
    "            with open(cache_path, 'rb') as f:\n",
    "                embedding = pickle.load(f)\n",
    "            return embedding\n",
    "        except Exception as e:\n",
    "            # Clean up corrupted cache file\n",
    "            try:\n",
    "                os.unlink(cache_path)\n",
    "            except:\n",
    "                pass\n",
    "    return None\n",
    "\n",
    "def save_embedding_to_local(text: str, model: str, embedding: List[float]):\n",
    "    \"\"\"Save embedding to local cache\"\"\"\n",
    "    cache_path = get_local_cache_path(text, model)\n",
    "\n",
    "    try:\n",
    "        with open(cache_path, 'wb') as f:\n",
    "            pickle.dump(embedding, f)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "def load_embedding(text: str, model: str) -> Optional[List[float]]:\n",
    "    \"\"\"Load cached embedding based on storage setting\"\"\"\n",
    "    if EMBEDDING_STORAGE == \"s3\":\n",
    "        return load_embedding_from_s3(text, model)\n",
    "    elif EMBEDDING_STORAGE == \"local\":\n",
    "        return load_embedding_from_local(text, model)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown EMBEDDING_STORAGE: {EMBEDDING_STORAGE}\")\n",
    "\n",
    "def save_embedding(text: str, model: str, embedding: List[float]):\n",
    "    \"\"\"Save embedding based on storage setting\"\"\"\n",
    "    if EMBEDDING_STORAGE == \"s3\":\n",
    "        return save_embedding_to_s3(text, model, embedding)\n",
    "    elif EMBEDDING_STORAGE == \"local\":\n",
    "        return save_embedding_to_local(text, model, embedding)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown EMBEDDING_STORAGE: {EMBEDDING_STORAGE}\")\n",
    "\n",
    "print(\"âœ… Embeddings cache ready (with retry logic)\")\n",
    "print(f\"   Storage: {EMBEDDING_STORAGE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8237e2ac",
   "metadata": {
    "id": "8237e2ac"
   },
   "source": [
    "## Embedding Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4938efe6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4938efe6",
    "outputId": "8393a3cc-4fdf-45cd-9159-2319b707fa32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Embedding generation ready\n"
     ]
    }
   ],
   "source": [
    "def get_embeddings_batch(texts: List[str], model: str = EMBEDDING_MODEL, batch_size: int = 1000) -> List[List[float]]:\n",
    "    \"\"\"Generate embeddings with caching (S3 or local based on EMBEDDING_STORAGE)\"\"\"\n",
    "    cleaned_texts = [clean_question(t) for t in texts]\n",
    "    embeddings = []\n",
    "    cache_hits = 0\n",
    "    api_calls = 0\n",
    "\n",
    "    print(f\"ðŸ”„ Processing {len(cleaned_texts)} texts...\")\n",
    "\n",
    "    for i in tqdm(range(0, len(cleaned_texts), batch_size), desc=\"Batches\"):\n",
    "        batch_texts = cleaned_texts[i:i+batch_size]\n",
    "        batch_embeddings = []\n",
    "        uncached_texts = []\n",
    "        uncached_indices = []\n",
    "\n",
    "        # Check cache (S3 or local based on setting)\n",
    "        for j, text in enumerate(batch_texts):\n",
    "            cached = load_embedding(text, model)\n",
    "            if cached:\n",
    "                batch_embeddings.append(cached)\n",
    "                cache_hits += 1\n",
    "            else:\n",
    "                batch_embeddings.append(None)\n",
    "                uncached_texts.append(text)\n",
    "                uncached_indices.append(j)\n",
    "\n",
    "        # Generate uncached embeddings\n",
    "        if uncached_texts:\n",
    "            try:\n",
    "                response = client.embeddings.create(model=model, input=uncached_texts)\n",
    "                new_embeddings = [d.embedding for d in response.data]\n",
    "                api_calls += len(uncached_texts)\n",
    "\n",
    "                for idx, emb in zip(uncached_indices, new_embeddings):\n",
    "                    batch_embeddings[idx] = emb\n",
    "                    save_embedding(batch_texts[idx], model, emb)\n",
    "            except Exception as e:\n",
    "                error_logger.log_error(\"Embeddings\", f\"Batch failed\", e)\n",
    "                for idx in uncached_indices:\n",
    "                    batch_embeddings[idx] = [0.0] * EMBEDDING_DIMENSIONS\n",
    "\n",
    "        embeddings.extend(batch_embeddings)\n",
    "\n",
    "    print(f\"âœ… Complete! Cache: {cache_hits}/{len(embeddings)} ({cache_hits/len(embeddings)*100:.1f}%), API: {api_calls}\")\n",
    "    return embeddings\n",
    "\n",
    "print(\"âœ… Embedding generation ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8868d0ba",
   "metadata": {
    "id": "8868d0ba"
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f68989f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 718
    },
    "id": "7f68989f",
    "outputId": "3cadab34-87dc-4b2f-b7fe-9be8e9fbdce1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Loading topics from Google Sheets...\n",
      "âœ… Loaded 120 topics from Google Sheets\n",
      "   Unique topics: 59, Unique subtopics: 118\n",
      "\n",
      "ðŸ“‚ Upload Langfuse CSV:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-cd6ce3d4-9394-4396-8a64-31a2663a1798\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-cd6ce3d4-9394-4396-8a64-31a2663a1798\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving langfuse_traces_01_25_26.csv to langfuse_traces_01_25_26.csv\n",
      "\n",
      "ðŸ“Š Loading Langfuse data from langfuse_traces_01_25_26.csv...\n",
      "ðŸ” Filtering malformed rows (containing 'kwargs' AND 'args')...\n",
      "âœ… Filtered 8 malformed rows (0.3%)\n",
      "   Remaining: 2892 rows\n",
      "ðŸ§¹ Cleaning 2892 Langfuse rows...\n",
      "âœ… Cleaned data: 2892 rows (0 dropped)\n",
      "   Columns: ['id', 'timestamp', 'input', 'is_acm_question', 'output', 'user_feedback', 'country', 'state', 'city', 'ip_address', 'user_language', 'is_suspicious']\n",
      "   Country data: 2826 rows with country info\n",
      "\n",
      "ðŸ”„ Syncing master file with S3...\n",
      "ðŸ“‚ Checking S3 for master file...\n",
      "âœ… Loaded master from S3: 23669 rows\n",
      "   Timestamp range: 2025-07-01T00:00:00+00:00 to 2026-01-02T11:33:17.412000+00:00\n",
      "ðŸ”€ Merging new data (2892 rows) with master (23669 rows)...\n",
      "   Combined: 26561 rows\n",
      "ðŸ§¹ Removing duplicates (same question + timestamp)...\n",
      "   Removed 0 duplicates\n",
      "   Deduplication criteria: input + timestamp\n",
      "   Final: 26561 rows\n",
      "â˜ï¸  Uploading to S3...\n",
      "âœ… Uploaded to S3: s3://byupathway-public/langfuse/langfuse_traces_master.parquet\n",
      "âœ… Master file synced: 26561 total rows\n",
      "\n",
      "ðŸ“Š DATA LOADED:\n",
      "   Topics: 120 (59 unique)\n",
      "   Questions: 26561 (from master file)\n",
      "   With metadata: country=14845, timestamp=26561\n",
      "   ACM questions: 801\n",
      "   Errors: 0\n",
      "   Dropped rows: 8\n"
     ]
    }
   ],
   "source": [
    "# Load topics from Google Sheets\n",
    "print(\"ðŸ“Š Loading topics from Google Sheets...\")\n",
    "topics_df = read_topics_from_google_sheets(GOOGLE_SHEETS_URL)\n",
    "topics_df = preprocess_dataframe(topics_df, 'question')\n",
    "\n",
    "# Upload Langfuse CSV\n",
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    print(\"\\nðŸ“‚ Upload Langfuse CSV:\")\n",
    "    uploaded = files.upload()\n",
    "    langfuse_filename = list(uploaded.keys())[0]\n",
    "else:\n",
    "    langfuse_filename = \"notebook/langfuse_traces.csv\"\n",
    "\n",
    "# Load Langfuse data\n",
    "print(f\"\\nðŸ“Š Loading Langfuse data from {langfuse_filename}...\")\n",
    "langfuse_df = pd.read_csv(langfuse_filename)\n",
    "\n",
    "# STEP 1: Filter malformed rows (FIRST STEP - remove kwargs/args errors)\n",
    "langfuse_df = filter_malformed_rows(langfuse_df)\n",
    "\n",
    "# STEP 2: Clean Langfuse data (also separates general feedback from chat traces)\n",
    "langfuse_clean, general_feedback_df = clean_langfuse_data(langfuse_df)\n",
    "\n",
    "# Store general feedback for later use (Streamlit Feedback page)\n",
    "if not general_feedback_df.empty:\n",
    "    print(f\"\\nðŸ“ General feedback entries stored: {len(general_feedback_df)}\")\n",
    "else:\n",
    "    print(f\"\\nðŸ“ No general feedback entries found\")\n",
    "\n",
    "# STEP 3: Sync with master file (S3 storage)\n",
    "master_df = sync_master_file(langfuse_clean)\n",
    "\n",
    "# Create questions dataframe with all metadata columns preserved\n",
    "required_cols = [\n",
    "    'id', 'input', 'timestamp', 'country', 'state', 'city',\n",
    "    'output', 'user_feedback', 'feedback_comment', 'user_language', 'is_suspicious',\n",
    "    'is_acm_question', 'latency', 'total_cost', 'session_id', 'user_id',\n",
    "    'tags', 'scores', 'release', 'role'\n",
    "]\n",
    "available_cols = [col for col in required_cols if col in master_df.columns]\n",
    "questions_df = master_df[available_cols].copy()\n",
    "questions_df = questions_df.rename(columns={'input': 'question'})\n",
    "questions_df = preprocess_dataframe(questions_df, 'question')\n",
    "\n",
    "print(f\"\\nðŸ“Š DATA LOADED:\")\n",
    "print(f\"   Topics: {len(topics_df)} ({topics_df['topic'].nunique()} unique)\")\n",
    "print(f\"   Questions: {len(questions_df)} (from master file)\")\n",
    "print(f\"   With metadata: country={questions_df['country'].notna().sum()}, timestamp={questions_df['timestamp'].notna().sum()}\")\n",
    "print(f\"   ACM questions: {questions_df['is_acm_question'].sum() if 'is_acm_question' in questions_df.columns else 'N/A'}\")\n",
    "print(f\"   With latency: {questions_df['latency'].notna().sum() if 'latency' in questions_df.columns else 'N/A'}\")\n",
    "print(f\"   With cost: {questions_df['total_cost'].notna().sum() if 'total_cost' in questions_df.columns else 'N/A'}\")\n",
    "print(f\"   With user_id: {questions_df['user_id'].notna().sum() if 'user_id' in questions_df.columns else 'N/A'}\")\n",
    "print(f\"   With session_id: {questions_df['session_id'].notna().sum() if 'session_id' in questions_df.columns else 'N/A'}\")\n",
    "print(f\"   With feedback reasons: {questions_df['feedback_comment'].notna().sum() if 'feedback_comment' in questions_df.columns else 'N/A'}\")\n",
    "print(f\"   Errors: {error_logger.get_summary()['total_errors']}\")\n",
    "print(f\"   Dropped rows: {error_logger.get_summary()['total_dropped_rows']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ef9941",
   "metadata": {
    "id": "b3ef9941"
   },
   "source": [
    "## Prepare Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c347fef",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3c347fef",
    "outputId": "20d0a193-e322-4630-c358-0cb8d2a589cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Full mode: 26561 questions\n",
      "ðŸ’° Estimated cost: $0.0267 (embeddings only)\n"
     ]
    }
   ],
   "source": [
    "if EVAL_MODE == \"sample\":\n",
    "    eval_df = questions_df.sample(n=min(SAMPLE_SIZE, len(questions_df)), random_state=RANDOM_SEED).copy()\n",
    "    print(f\"ðŸ“ Sample mode: {len(eval_df)} questions\")\n",
    "else:\n",
    "    eval_df = questions_df.copy()\n",
    "    print(f\"ðŸ“ Full mode: {len(eval_df)} questions\")\n",
    "\n",
    "# Cost estimate\n",
    "total_tokens = (len(topics_df) + len(eval_df)) * 50\n",
    "embedding_cost = (total_tokens / 1_000_000) * 0.02\n",
    "print(f\"ðŸ’° Estimated cost: ${embedding_cost:.4f} (embeddings only)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e251a632",
   "metadata": {
    "id": "e251a632"
   },
   "source": [
    "## Similarity Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adab14e9",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "adab14e9",
    "outputId": "1ecd96b4-6ca2-4aec-9317-37d8bbd936ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¯ Similarity Classification (threshold: 0.7)\n",
      "ðŸ“Š Generating topic embeddings...\n",
      "ðŸ”„ Processing 120 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Complete! Cache: 0/120 (0.0%), API: 120\n",
      "ðŸ“Š Generating question embeddings...\n",
      "ðŸ”„ Processing 26561 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:52<00:00,  1.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Complete! Cache: 4693/26561 (17.7%), API: 21868\n",
      "ðŸ” Classifying 26561 questions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26561/26561 [16:13<00:00, 27.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Classification complete:\n",
      "   Similar: 3942 (14.8%)\n",
      "   Remaining: 22619 (85.2%)\n"
     ]
    }
   ],
   "source": [
    "def classify_by_similarity(questions_df: pd.DataFrame, topics_df: pd.DataFrame, threshold: float) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Classify questions by similarity to existing topics - preserves all metadata columns\"\"\"\n",
    "\n",
    "    print(f\"\\nðŸŽ¯ Similarity Classification (threshold: {threshold})\")\n",
    "\n",
    "    # Generate embeddings\n",
    "    print(f\"ðŸ“Š Generating topic embeddings...\")\n",
    "    topic_embeddings = get_embeddings_batch(topics_df['question'].tolist())\n",
    "    topics_df = topics_df.copy()\n",
    "    topics_df['embedding'] = topic_embeddings\n",
    "\n",
    "    print(f\"ðŸ“Š Generating question embeddings...\")\n",
    "    question_embeddings = get_embeddings_batch(questions_df['question'].tolist())\n",
    "\n",
    "    # Classify\n",
    "    similar = []\n",
    "    remaining = []\n",
    "\n",
    "    print(f\"ðŸ” Classifying {len(questions_df)} questions...\")\n",
    "    for idx, (_, row) in enumerate(tqdm(questions_df.iterrows(), total=len(questions_df))):\n",
    "        question = row['question']\n",
    "        q_emb = question_embeddings[idx]\n",
    "\n",
    "        if not q_emb or len(q_emb) != EMBEDDING_DIMENSIONS:\n",
    "            row_data = row.to_dict()\n",
    "            row_data['embedding'] = [0.0]*EMBEDDING_DIMENSIONS\n",
    "            remaining.append(row_data)\n",
    "            continue\n",
    "\n",
    "        best_sim = 0\n",
    "        best_match = None\n",
    "\n",
    "        for _, topic_row in topics_df.iterrows():\n",
    "            t_emb = topic_row['embedding']\n",
    "            if t_emb and len(t_emb) == EMBEDDING_DIMENSIONS:\n",
    "                sim = 1 - cosine(q_emb, t_emb)\n",
    "                if sim > best_sim:\n",
    "                    best_sim = sim\n",
    "                    best_match = topic_row\n",
    "\n",
    "        if best_sim >= threshold and best_match is not None:\n",
    "            row_data = row.to_dict()\n",
    "            row_data['matched_topic'] = best_match['topic']\n",
    "            row_data['matched_subtopic'] = best_match['subtopic']\n",
    "            row_data['similarity_score'] = best_sim\n",
    "            similar.append(row_data)\n",
    "        else:\n",
    "            row_data = row.to_dict()\n",
    "            row_data['embedding'] = q_emb\n",
    "            remaining.append(row_data)\n",
    "\n",
    "    similar_df = pd.DataFrame(similar)\n",
    "    remaining_df = pd.DataFrame(remaining)\n",
    "\n",
    "    print(f\"\\nâœ… Classification complete:\")\n",
    "    print(f\"   Similar: {len(similar_df)} ({len(similar_df)/len(questions_df)*100:.1f}%)\")\n",
    "    print(f\"   Remaining: {len(remaining_df)} ({len(remaining_df)/len(questions_df)*100:.1f}%)\")\n",
    "\n",
    "    return similar_df, remaining_df\n",
    "\n",
    "similar_df, remaining_df = classify_by_similarity(eval_df, topics_df, SIMILARITY_THRESHOLD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d352ac43",
   "metadata": {
    "id": "d352ac43"
   },
   "source": [
    "## Clustering for New Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b483e59f",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "b483e59f",
    "outputId": "0e1abea0-de20-4337-b354-5c8eb380965c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¯ Clustering 22619 remaining questions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/hdbscan/robust_single_linkage_.py:175: SyntaxWarning: invalid escape sequence '\\{'\n",
      "  $max \\{ core_k(a), core_k(b), 1/\\alpha d(a,b) \\}$.\n",
      "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ UMAP reduction to 5 dimensions...\n",
      "ðŸ”„ HDBSCAN clustering (min_size=3)...\n",
      "âœ… Found 1694 clusters, 5174 noise points\n",
      "âœ… Clustered 17445 questions into 1694 topics\n"
     ]
    }
   ],
   "source": [
    "clustered_df = None\n",
    "topic_model = None\n",
    "\n",
    "if len(remaining_df) > 0:\n",
    "    print(f\"\\nðŸŽ¯ Clustering {len(remaining_df)} remaining questions...\")\n",
    "\n",
    "    from umap import UMAP\n",
    "    from hdbscan import HDBSCAN\n",
    "    from bertopic import BERTopic\n",
    "\n",
    "    embeddings = np.array(remaining_df['embedding'].tolist())\n",
    "\n",
    "    # UMAP\n",
    "    print(f\"ðŸ”„ UMAP reduction to {UMAP_N_COMPONENTS} dimensions...\")\n",
    "    umap_model = UMAP(n_components=UMAP_N_COMPONENTS, min_dist=0.0, metric='cosine', random_state=RANDOM_SEED)\n",
    "    reduced = umap_model.fit_transform(embeddings)\n",
    "\n",
    "    # HDBSCAN\n",
    "    print(f\"ðŸ”„ HDBSCAN clustering (min_size={HDBSCAN_MIN_CLUSTER_SIZE})...\")\n",
    "    hdbscan_model = HDBSCAN(min_cluster_size=HDBSCAN_MIN_CLUSTER_SIZE, metric=\"euclidean\", cluster_selection_method=\"eom\")\n",
    "    clusters = hdbscan_model.fit_predict(reduced)\n",
    "\n",
    "    n_clusters = len([c for c in np.unique(clusters) if c != -1])\n",
    "    n_noise = sum(clusters == -1)\n",
    "    print(f\"âœ… Found {n_clusters} clusters, {n_noise} noise points\")\n",
    "\n",
    "    if n_clusters > 0:\n",
    "        # BERTopic\n",
    "        topic_model = BERTopic(embedding_model=None, umap_model=umap_model, hdbscan_model=hdbscan_model, verbose=False)\n",
    "        topics, probs = topic_model.fit_transform(remaining_df['question'].tolist(), embeddings)\n",
    "\n",
    "        clustered_df = remaining_df.copy()\n",
    "        clustered_df['cluster_id'] = clusters\n",
    "        clustered_df['topic_id'] = topics\n",
    "        clustered_df = clustered_df[clustered_df['cluster_id'] != -1]\n",
    "\n",
    "        print(f\"âœ… Clustered {len(clustered_df)} questions into {n_clusters} topics\")\n",
    "else:\n",
    "    print(\"\\nâœ… All questions matched existing topics - no clustering needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cf39bc",
   "metadata": {
    "id": "14cf39bc"
   },
   "source": [
    "## Generate Topic Names with GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f310a6b",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "3f310a6b",
    "outputId": "f24099c0-06ed-4437-b834-57d7df6e6710"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ¤– Generating topic names with gpt-5-nano...\n",
      "âœ… Generated 1669 topic names\n",
      "   TR letter grade meaning (34 questions)\n",
      "   Transfer Credit and TR Grades (7 questions)\n",
      "   Haitian Student Tuition and Scholarships (17 questions)\n",
      "   PathwayConnect Absence Corrections and Global Services (20 questions)\n",
      "   Inside Language Model: Forbidden Questions (8 questions)\n"
     ]
    }
   ],
   "source": [
    "topic_names = {}\n",
    "\n",
    "if clustered_df is not None and len(clustered_df) > 0:\n",
    "    print(f\"\\nðŸ¤– Generating topic names with {GPT_MODEL}...\")\n",
    "\n",
    "    async def generate_topic_name(questions: List[str], keywords: str = \"\") -> str:\n",
    "        \"\"\"Generate a topic name using GPT-5 for a cluster of questions\"\"\"\n",
    "\n",
    "        # Limit to top 10 questions for context (like insights)\n",
    "        sample_questions = questions[:10]\n",
    "        questions_text = \"\\n\".join([f\"- {q}\" for q in sample_questions])\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "Based on the following student questions and keywords, generate a concise, descriptive topic name.\n",
    "\n",
    "QUESTIONS:\n",
    "{questions_text}\n",
    "\n",
    "KEYWORDS: {keywords}\n",
    "\n",
    "Instructions:\n",
    "- Your answer must be ONLY the topic name (2â€“8 words), no extra text.\n",
    "- It should clearly describe the shared theme of the questions.\n",
    "- Avoid generic labels like \"General Questions\" or \"Miscellaneous.\"\n",
    "- Do not include \"Topic name:\" or quotation marks.\n",
    "- Use simple, natural English that sounds clear to a student or teacher.\n",
    "\n",
    "Example:\n",
    "Questions:\n",
    "- When does registration open?\n",
    "- What are the fall 2025 enrollment deadlines?\n",
    "Keywords: registration, deadlines\n",
    "\n",
    "Topic name: Fall 2025 Registration Deadlines\n",
    "\n",
    "Now generate the topic name for the questions above:\n",
    "\"\"\"\n",
    "\n",
    "        try:\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert at creating clear, descriptive topic names for student question categories.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "\n",
    "            # GPT-5 specific configuration (NO temperature parameter!)\n",
    "            response = await async_client.chat.completions.create(\n",
    "                model=GPT_MODEL,\n",
    "                messages=messages,\n",
    "                max_completion_tokens=1000  # Use max_completion_tokens for GPT-5, not max_tokens\n",
    "            )\n",
    "\n",
    "            topic_name = response.choices[0].message.content.strip()\n",
    "\n",
    "            # Clean up the response\n",
    "            topic_name = topic_name.replace(\"Topic name:\", \"\").strip()\n",
    "            topic_name = topic_name.strip('\\\"\\'')\n",
    "\n",
    "            if not topic_name:\n",
    "                topic_name = f\"Topic: {keywords[:50]}\" if keywords else f\"Question Group {hash(str(questions[:3])) % 1000}\"\n",
    "\n",
    "            return topic_name\n",
    "\n",
    "        except Exception as e:\n",
    "            error_logger.log_error(\"TopicNaming\", f\"GPT failed: {str(e)}\", e)\n",
    "            # Fallback to keyword-based name\n",
    "            fallback_name = f\"Topic: {keywords[:50]}\" if keywords else f\"Question Group {hash(str(questions[:3])) % 1000}\"\n",
    "            return fallback_name\n",
    "\n",
    "    async def process_all_clusters():\n",
    "        tasks = []\n",
    "        cluster_ids = []\n",
    "\n",
    "        for cluster_id, group in clustered_df.groupby('cluster_id'):\n",
    "            questions = group['question'].tolist()\n",
    "            # Extract keywords from BERTopic if available\n",
    "            keywords = group['topic_keywords'].iloc[0] if 'topic_keywords' in group.columns else \"\"\n",
    "\n",
    "            tasks.append(generate_topic_name(questions, keywords))\n",
    "            cluster_ids.append(cluster_id)\n",
    "\n",
    "        names = await asyncio.gather(*tasks)\n",
    "        return dict(zip(cluster_ids, names))\n",
    "\n",
    "    topic_names = await process_all_clusters()\n",
    "    clustered_df['topic_name'] = clustered_df['cluster_id'].map(topic_names)\n",
    "\n",
    "    print(f\"âœ… Generated {len(topic_names)} topic names\")\n",
    "    for cid, name in list(topic_names.items())[:5]:\n",
    "        count = len(clustered_df[clustered_df['cluster_id'] == cid])\n",
    "        print(f\"   {name} ({count} questions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30620d7",
   "metadata": {
    "id": "f30620d7"
   },
   "source": [
    "## Generate Output Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc0920e",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "cdc0920e",
    "outputId": "811087a6-0e33-4cca-971b-e0875e680d1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ Generating output files...\n",
      "âœ… similar_questions_20260126_104758.parquet: 3942 rows\n",
      "âœ… new_topics_20260126_104758.parquet: 1669 rows\n",
      "âœ… pathway_questions_review_20260126_104758.parquet: 26357 rows\n",
      "   Columns: ['question', 'topic_name', 'classification', 'confidence', 'timestamp', 'country', 'state', 'city', 'output', 'user_feedback', 'ip_address', 'user_language', 'is_suspicious', 'is_acm_question']\n",
      "   With country data: 14823 rows\n",
      "   ACM questions: 792\n",
      "âœ… topic_distribution_20260126_104758.parquet: 1750 rows\n",
      "âœ… error_log_20260126_104758.json: error summary\n",
      "\n",
      "ðŸ“¦ OUTPUT FILES GENERATED:\n",
      "   1. similar_questions_20260126_104758.parquet - Questions matched to existing topics\n",
      "   2. new_topics_20260126_104758.parquet - New topic clusters discovered\n",
      "   3. pathway_questions_review_20260126_104758.parquet - Complete review with ALL METADATA\n",
      "   4. topic_distribution_20260126_104758.parquet - Topic distribution analytics\n",
      "   5. error_log_20260126_104758.json - Error log\n"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "print(f\"\\nðŸ“ Generating output files...\")\n",
    "\n",
    "# File 1: Similar questions\n",
    "file1 = f\"similar_questions_{timestamp}.parquet\"\n",
    "if len(similar_df) > 0:\n",
    "    output1 = similar_df[['question', 'matched_topic', 'matched_subtopic', 'similarity_score']].copy()\n",
    "    output1.columns = ['question', 'existing_topic', 'existing_subtopic', 'similarity_score']\n",
    "    output1 = output1.sort_values('similarity_score', ascending=False)\n",
    "else:\n",
    "    output1 = pd.DataFrame(columns=['question', 'existing_topic', 'existing_subtopic', 'similarity_score'])\n",
    "\n",
    "# Add metadata\n",
    "output1.attrs['metadata'] = {\n",
    "    'timestamp': timestamp,\n",
    "    'threshold': SIMILARITY_THRESHOLD,\n",
    "    'total_questions': len(output1),\n",
    "    'default_visible_columns': ['question', 'existing_topic', 'existing_subtopic', 'similarity_score']\n",
    "}\n",
    "output1.to_parquet(file1)\n",
    "print(f\"âœ… {file1}: {len(output1)} rows\")\n",
    "\n",
    "# File 2: New topics\n",
    "file2 = f\"new_topics_{timestamp}.parquet\"\n",
    "if clustered_df is not None and len(clustered_df) > 0:\n",
    "    cluster_summary = clustered_df.groupby('cluster_id').agg({\n",
    "        'topic_name': 'first',\n",
    "        'question': ['first', 'count']\n",
    "    }).reset_index()\n",
    "    cluster_summary.columns = ['cluster_id', 'topic_name', 'representative_question', 'question_count']\n",
    "    output2 = cluster_summary[['topic_name', 'representative_question', 'question_count']].sort_values('question_count', ascending=False)\n",
    "else:\n",
    "    output2 = pd.DataFrame(columns=['topic_name', 'representative_question', 'question_count'])\n",
    "\n",
    "output2.attrs['metadata'] = {\n",
    "    'timestamp': timestamp,\n",
    "    'total_topics': len(output2),\n",
    "    'default_visible_columns': ['topic_name', 'representative_question', 'question_count']\n",
    "}\n",
    "output2.to_parquet(file2)\n",
    "print(f\"âœ… {file2}: {len(output2)} rows\")\n",
    "\n",
    "# File 3: All questions review WITH METADATA\n",
    "file3 = f\"pathway_questions_review_{timestamp}.parquet\"\n",
    "review_data = []\n",
    "\n",
    "# Metadata columns to preserve (including new fields: feedback_comment, latency, cost, session/user tracking)\n",
    "metadata_cols = [\n",
    "    'timestamp', 'country', 'state', 'city', 'output', 'user_feedback', 'feedback_comment',\n",
    "    'user_language', 'is_suspicious', 'is_acm_question',\n",
    "    'latency', 'total_cost', 'session_id', 'user_id', 'tags', 'scores',\n",
    "    'release', 'role'\n",
    "]\n",
    "\n",
    "if len(similar_df) > 0:\n",
    "    for _, row in similar_df.iterrows():\n",
    "        record = {\n",
    "            'question': row['question'],\n",
    "            'topic_name': f\"{row['matched_topic']} | {row['matched_subtopic']}\",\n",
    "            'classification': 'existing',\n",
    "            'confidence': row['similarity_score']\n",
    "        }\n",
    "        # Add metadata columns\n",
    "        for col in metadata_cols:\n",
    "            record[col] = row.get(col, None)\n",
    "        review_data.append(record)\n",
    "\n",
    "if clustered_df is not None and len(clustered_df) > 0:\n",
    "    for _, row in clustered_df.iterrows():\n",
    "        record = {\n",
    "            'question': row['question'],\n",
    "            'topic_name': row['topic_name'],\n",
    "            'classification': 'new',\n",
    "            'confidence': 0.5\n",
    "        }\n",
    "        # Add metadata columns\n",
    "        for col in metadata_cols:\n",
    "            record[col] = row.get(col, None)\n",
    "        review_data.append(record)\n",
    "\n",
    "if len(remaining_df) > len(clustered_df) if clustered_df is not None else len(remaining_df) > 0:\n",
    "    clustered_questions = set(clustered_df['question']) if clustered_df is not None else set()\n",
    "    for _, row in remaining_df.iterrows():\n",
    "        if row['question'] not in clustered_questions:\n",
    "            record = {\n",
    "                'question': row['question'],\n",
    "                'topic_name': 'Other',\n",
    "                'classification': 'uncategorized',\n",
    "                'confidence': 0.0\n",
    "            }\n",
    "            # Add metadata columns\n",
    "            for col in metadata_cols:\n",
    "                record[col] = row.get(col, None)\n",
    "            review_data.append(record)\n",
    "\n",
    "output3 = pd.DataFrame(review_data)\n",
    "output3.attrs['metadata'] = {\n",
    "    'timestamp': timestamp,\n",
    "    'total_questions': len(output3),\n",
    "    'default_visible_columns': ['question', 'timestamp', 'country', 'state', 'topic_name', 'classification', 'is_acm_question']\n",
    "}\n",
    "output3.to_parquet(file3)\n",
    "print(f\"âœ… {file3}: {len(output3)} rows\")\n",
    "print(f\"   Columns: {list(output3.columns)}\")\n",
    "print(f\"   With country data: {output3['country'].notna().sum()} rows\")\n",
    "print(f\"   ACM questions: {output3['is_acm_question'].sum() if 'is_acm_question' in output3.columns else 'N/A'}\")\n",
    "print(f\"   With latency: {output3['latency'].notna().sum() if 'latency' in output3.columns else 'N/A'}\")\n",
    "print(f\"   With cost: {output3['total_cost'].notna().sum() if 'total_cost' in output3.columns else 'N/A'}\")\n",
    "print(f\"   With user_id: {output3['user_id'].notna().sum() if 'user_id' in output3.columns else 'N/A'}\")\n",
    "print(f\"   With feedback reasons: {output3['feedback_comment'].notna().sum() if 'feedback_comment' in output3.columns else 'N/A'}\")\n",
    "\n",
    "# File 4: Topic distribution analytics\n",
    "file4 = f\"topic_distribution_{timestamp}.parquet\"\n",
    "topic_dist = output3.groupby(['topic_name', 'classification']).size().reset_index(name='count')\n",
    "topic_dist = topic_dist.sort_values('count', ascending=False)\n",
    "topic_dist.attrs['metadata'] = {\n",
    "    'timestamp': timestamp,\n",
    "    'total_topics': len(topic_dist),\n",
    "    'default_visible_columns': ['topic_name', 'classification', 'count']\n",
    "}\n",
    "topic_dist.to_parquet(file4)\n",
    "print(f\"âœ… {file4}: {len(topic_dist)} rows\")\n",
    "\n",
    "# File 5: General feedback (separate from questions)\n",
    "file5 = f\"general_feedback_{timestamp}.parquet\"\n",
    "if not general_feedback_df.empty:\n",
    "    general_feedback_df.to_parquet(file5)\n",
    "    print(f\"âœ… {file5}: {len(general_feedback_df)} rows\")\n",
    "else:\n",
    "    # Create empty parquet with expected schema\n",
    "    empty_fb = pd.DataFrame(columns=['id', 'name', 'input', 'output', 'timestamp', 'user_id', 'session_id', 'tags'])\n",
    "    empty_fb.to_parquet(file5)\n",
    "    print(f\"âœ… {file5}: 0 rows (empty)\")\n",
    "\n",
    "# Error log\n",
    "error_log_file = f\"error_log_{timestamp}.json\"\n",
    "with open(error_log_file, 'w') as f:\n",
    "    json.dump(error_logger.get_summary(), f, indent=2)\n",
    "print(f\"âœ… {error_log_file}: error summary\")\n",
    "\n",
    "# Create output_files list for S3 upload\n",
    "output_files = [file1, file2, file3, file4, file5, error_log_file]\n",
    "\n",
    "print(f\"\\nðŸ“¦ OUTPUT FILES GENERATED:\")\n",
    "print(f\"   1. {file1} - Questions matched to existing topics\")\n",
    "print(f\"   2. {file2} - New topic clusters discovered\")\n",
    "print(f\"   3. {file3} - Complete review with ALL METADATA\")\n",
    "print(f\"   4. {file4} - Topic distribution analytics\")\n",
    "print(f\"   5. {file5} - General feedback submissions\")\n",
    "print(f\"   6. {error_log_file} - Error log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7625c6",
   "metadata": {
    "id": "9a7625c6"
   },
   "source": [
    "## Upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e9af3a",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "a9e9af3a",
    "outputId": "d469e866-27ec-42be-eac5-9a9c0c54b24d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â˜ï¸  Uploading to S3 (without public ACL)...\n",
      "ðŸ—‘ï¸  Deleted 5 objects from s3://byupathway-public/topic-modeling-data\n",
      "ðŸ“¤ Uploading similar_questions_20260126_104758.parquet (153,128 bytes)...\n",
      "âœ… Uploaded to S3: s3://byupathway-public/topic-modeling-data/similar_questions_20260126_104758.parquet\n",
      "   âœ… Success: s3://byupathway-public/topic-modeling-data/similar_questions_20260126_104758.parquet\n",
      "ðŸ“¤ Uploading new_topics_20260126_104758.parquet (112,022 bytes)...\n",
      "âœ… Uploaded to S3: s3://byupathway-public/topic-modeling-data/new_topics_20260126_104758.parquet\n",
      "   âœ… Success: s3://byupathway-public/topic-modeling-data/new_topics_20260126_104758.parquet\n",
      "ðŸ“¤ Uploading pathway_questions_review_20260126_104758.parquet (6,862,887 bytes)...\n",
      "âœ… Uploaded to S3: s3://byupathway-public/topic-modeling-data/pathway_questions_review_20260126_104758.parquet\n",
      "   âœ… Success: s3://byupathway-public/topic-modeling-data/pathway_questions_review_20260126_104758.parquet\n",
      "ðŸ“¤ Uploading topic_distribution_20260126_104758.parquet (49,033 bytes)...\n",
      "âœ… Uploaded to S3: s3://byupathway-public/topic-modeling-data/topic_distribution_20260126_104758.parquet\n",
      "   âœ… Success: s3://byupathway-public/topic-modeling-data/topic_distribution_20260126_104758.parquet\n",
      "ðŸ“¤ Uploading error_log_20260126_104758.json (1,832 bytes)...\n",
      "âœ… Uploaded to S3: s3://byupathway-public/topic-modeling-data/error_log_20260126_104758.json\n",
      "   âœ… Success: s3://byupathway-public/topic-modeling-data/error_log_20260126_104758.json\n",
      "\n",
      "ðŸ“Š UPLOAD SUMMARY:\n",
      "   âœ… Successful: 5/5\n",
      "   âŒ Failed: 0/5\n",
      "\n",
      "âœ… Uploaded files (private - not publicly accessible):\n",
      "   s3://byupathway-public/topic-modeling-data/similar_questions_20260126_104758.parquet\n",
      "   s3://byupathway-public/topic-modeling-data/new_topics_20260126_104758.parquet\n",
      "   s3://byupathway-public/topic-modeling-data/pathway_questions_review_20260126_104758.parquet\n",
      "   s3://byupathway-public/topic-modeling-data/topic_distribution_20260126_104758.parquet\n",
      "   s3://byupathway-public/topic-modeling-data/error_log_20260126_104758.json\n",
      "\n",
      "ðŸ’¡ TIP: Files are uploaded but not public. Your Streamlit app can access them with AWS credentials.\n"
     ]
    }
   ],
   "source": [
    "# Skip ACL if you don't have PutObjectAcl permission\n",
    "# Use this if you can upload without ACL but not with public-read\n",
    "\n",
    "print(f\"\\nâ˜ï¸  Uploading to S3 (without public ACL)...\")\n",
    "\n",
    "# Delete old files\n",
    "try:\n",
    "    delete_s3_folder(S3_OUTPUT_PREFIX)\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  Could not delete old files: {e}\")\n",
    "\n",
    "# Upload new files WITHOUT public-read ACL\n",
    "uploaded = []\n",
    "failed = []\n",
    "\n",
    "for filepath in output_files:\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"âŒ File not found: {filepath}\")\n",
    "        failed.append(filepath)\n",
    "        continue\n",
    "\n",
    "    file_size = os.path.getsize(filepath)\n",
    "    print(f\"ðŸ“¤ Uploading {filepath} ({file_size:,} bytes)...\")\n",
    "\n",
    "    s3_key = f\"{S3_OUTPUT_PREFIX}/{filepath}\"\n",
    "\n",
    "    try:\n",
    "        # Use public=False to skip ACL (if you don't have PutObjectAcl permission)\n",
    "        if upload_to_s3(filepath, s3_key, public=False):\n",
    "            # Note: URL won't be publicly accessible without ACL\n",
    "            url = f\"s3://{S3_BUCKET}/{s3_key}\"\n",
    "            uploaded.append(url)\n",
    "            print(f\"   âœ… Success: {url}\")\n",
    "        else:\n",
    "            failed.append(filepath)\n",
    "            print(f\"   âŒ Failed: {filepath}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Exception: {str(e)}\")\n",
    "        failed.append(filepath)\n",
    "\n",
    "print(f\"\\nðŸ“Š UPLOAD SUMMARY:\")\n",
    "print(f\"   âœ… Successful: {len(uploaded)}/{len(output_files)}\")\n",
    "print(f\"   âŒ Failed: {len(failed)}/{len(output_files)}\")\n",
    "\n",
    "if uploaded:\n",
    "    print(f\"\\nâœ… Uploaded files (private - not publicly accessible):\")\n",
    "    for url in uploaded:\n",
    "        print(f\"   {url}\")\n",
    "    print(f\"\\nðŸ’¡ TIP: Files are uploaded but not public. Your Streamlit app can access them with AWS credentials.\")\n",
    "\n",
    "if failed:\n",
    "    print(f\"\\nâŒ Failed files:\")\n",
    "    for f in failed:\n",
    "        print(f\"   {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7865c4b",
   "metadata": {
    "id": "b7865c4b"
   },
   "source": [
    "## Analysis & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d64886",
   "metadata": {
    "id": "36d64886"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Hybrid Topic Discovery Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Processing pipeline\n",
    "pipeline = ['Total', 'Similar', 'New Topics', 'Uncategorized']\n",
    "counts = [\n",
    "    len(eval_df),\n",
    "    len(similar_df),\n",
    "    len(clustered_df) if clustered_df is not None else 0,\n",
    "    len(eval_df) - len(similar_df) - (len(clustered_df) if clustered_df is not None else 0)\n",
    "]\n",
    "axes[0,0].bar(pipeline, counts, color=['lightblue', 'lightgreen', 'orange', 'lightcoral'])\n",
    "axes[0,0].set_title('Processing Results')\n",
    "axes[0,0].set_ylabel('Questions')\n",
    "for i, (label, count) in enumerate(zip(pipeline, counts)):\n",
    "    axes[0,0].text(i, count + max(counts)*0.01, f\"{count}\\n({count/len(eval_df)*100:.1f}%)\", ha='center')\n",
    "\n",
    "# 2. Similarity distribution\n",
    "if len(similar_df) > 0:\n",
    "    axes[0,1].hist(similar_df['similarity_score'], bins=20, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "    axes[0,1].axvline(SIMILARITY_THRESHOLD, color='red', linestyle='--', label=f'Threshold: {SIMILARITY_THRESHOLD}')\n",
    "    axes[0,1].set_xlabel('Similarity Score')\n",
    "    axes[0,1].set_ylabel('Count')\n",
    "    axes[0,1].set_title('Similarity Distribution')\n",
    "    axes[0,1].legend()\n",
    "else:\n",
    "    axes[0,1].text(0.5, 0.5, 'No similar questions', ha='center', va='center', transform=axes[0,1].transAxes)\n",
    "    axes[0,1].set_title('Similarity Distribution')\n",
    "\n",
    "# 3. Cluster sizes\n",
    "if clustered_df is not None and len(clustered_df) > 0:\n",
    "    cluster_sizes = clustered_df['cluster_id'].value_counts().values\n",
    "    axes[1,0].hist(cluster_sizes, bins=min(20, len(cluster_sizes)), alpha=0.7, color='orange', edgecolor='black')\n",
    "    axes[1,0].set_xlabel('Cluster Size')\n",
    "    axes[1,0].set_ylabel('Count')\n",
    "    axes[1,0].set_title('New Topic Sizes')\n",
    "else:\n",
    "    axes[1,0].text(0.5, 0.5, 'No clusters', ha='center', va='center', transform=axes[1,0].transAxes)\n",
    "    axes[1,0].set_title('New Topic Sizes')\n",
    "\n",
    "# 4. Topic distribution pie\n",
    "pie_data = output3['classification'].value_counts()\n",
    "if len(pie_data) > 0:\n",
    "    axes[1,1].pie(pie_data.values, labels=pie_data.index, autopct='%1.1f%%', startangle=90)\n",
    "    axes[1,1].set_title('Classification Distribution')\n",
    "else:\n",
    "    axes[1,1].text(0.5, 0.5, 'No data', ha='center', va='center', transform=axes[1,1].transAxes)\n",
    "    axes[1,1].set_title('Classification Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š SUMMARY:\")\n",
    "print(f\"   Total processed: {len(eval_df)}\")\n",
    "print(f\"   Similar to existing: {len(similar_df)} ({len(similar_df)/len(eval_df)*100:.1f}%)\")\n",
    "print(f\"   New topics: {len(topic_names)}\")\n",
    "print(f\"   Errors: {error_logger.get_summary()['total_errors']}\")\n",
    "print(f\"   Warnings: {error_logger.get_summary()['total_warnings']}\")\n",
    "print(f\"\\nâœ… COMPLETE! Files uploaded to S3.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
